from ..serve import LLMServe, CHAT_MODE, RAGLLMServe
from typing import Literal, List
from .inference import InferenceSession


class LLamaLLMServe(LLMServe):
    def __init__(
            self,
            inference_session: InferenceSession,
            **kwargs
    ):
        self.inference_session = inference_session
        super().__init__(**kwargs)

    def sample(
            self,
            prompt: str,
            history: List[List[str]],
            system_prompt: str,
            mode: CHAT_MODE = CHAT_MODE[-1],
            max_length: int = 8192,
            max_new_tokens: int = 4096,
            temperature: float = 0.8,
            top_p: float = 0.9,
            top_k: int = 50
    ):
        """
        The sample function is the main entry point for a user to interact with the model.
        It takes in a prompt, which can be any string, and returns an iterator over
        strings that are generated by the model.
        The sample function also takes in some optional arguments:

        :param self: Refer to the current object
        :param prompt: str: Pass in the text that you want to generate a response for
        :param history: List[List[str]]: Keep track of the conversation history
        :param system_prompt: str: the model system prompt.
        :param mode: str: represent the mode that model inference be used in (e.g. chat or instruction)
        :param max_length: int: Maximum Length for model
        :param max_new_tokens: int: Limit the number of tokens in a response
        :param temperature: float: Control the randomness of the generated text
        :param top_p: float: Control the probability of sampling from the top k tokens
        :param top_k: int: Control the number of candidates that are considered for each token
        :return: A generator that yields the next token in the sequence
        """

        assert mode in CHAT_MODE, "Requested Mode is not in Available Modes"
        if mode == "Instruction":
            history = []
        string = self.interactor.format_message(
            prompt=prompt,
            history=history,
            system_message=None if system_prompt == "" else system_prompt,
            prefix=self.interactor.get_prefix_prompt() if self.use_prefix_for_interactor else None,
        )
        history.append([prompt, ""])
        total_response = ""
        for response in self.inference_session.generate(
                prompt=string,
                top_k=top_k,
                top_p=top_p,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
        ):
            total_response += response.predictions.text
            history[-1][-1] = total_response
            yield "", history


class LLamaRAGLLMServe(RAGLLMServe):
    def __init__(
            self,
            inference_session: InferenceSession,
            **kwargs
    ):
        self.inference_session = inference_session
        super().__init__(**kwargs)

    def sample(
            self,
            prompt: str,
            history: List[List[str]],
            system_prompt: str,
            mode: Literal["Chat", "Instruction"] = "Instruction",
            max_length: int = 8192,
            max_new_tokens: int = 4096,
            temperature: float = 0.8,
            top_p: float = 0.9,
            top_k: int = 50
    ):
        """
        The sample function is the main entry point for a user to interact with the model.
        It takes in a prompt, which can be any string, and returns an iterator over
        strings that are generated by the model.
        The sample function also takes in some optional arguments:

        :param self: Refer to the current object
        :param prompt: str: Pass in the text that you want to generate a response for
        :param history: List[List[str]]: Keep track of the conversation history
        :param system_prompt: str: the model system prompt.
        :param mode: str: represent the mode that model inference be used in (e.g. chat or instruction)
        :param max_length: int: Maximum Length for model
        :param max_new_tokens: int: Limit the number of tokens in a response
        :param temperature: float: Control the randomness of the generated text
        :param top_p: float: Control the probability of sampling from the top k tokens
        :param top_k: int: Control the number of candidates that are considered for each token
        :return: A generator that yields the next token in the sequence
        """

        assert mode in ["Chat", "Instruction"], "Requested Mode is not in Available Modes"
        if mode == "Instruction":
            history = []

        history.append([prompt, ""])
        prompt = self.rag_search(
            prompt
        )
        string = self.interactor.format_message(
            prompt=prompt,
            history=history,
            system_message=None if system_prompt == "" else system_prompt,
            prefix=self.interactor.get_prefix_prompt() if self.use_prefix_for_interactor else None,
        )

        total_response = ""
        for response in self.inference_session.generate(
                prompt=string,
                top_k=top_k,
                top_p=top_p,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
        ):
            total_response += response.predictions.text
            history[-1][-1] = total_response
            yield '', history
