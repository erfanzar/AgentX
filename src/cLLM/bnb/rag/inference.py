import warnings
from threading import Thread
from typing import Literal, List, Optional
from sentence_transformers import SentenceTransformer
import faiss
from transformers import TextIteratorStreamer

from ..gradio.gradio_interface import BNBLLMServe


def search(
        query: str,
        index: faiss.IndexFlatL2,
        embedding: SentenceTransformer,
        text_snippets: list[str],
        k: int,
):
    dists, ind = index.search(
        embedding.encode([query]),
        k=k
    )
    ind = ind[0].tolist()
    return [text_snippets[i] for i in ind]


class BNBRAGLLMServe(BNBLLMServe):
    index: Optional[faiss.IndexFlatL2] = None
    embedding: Optional[SentenceTransformer] = None
    text_snippets: Optional[list[str]] = None
    rag_top_k: Optional[int] = 3
    rag_qa_base_question: Optional[str] = None

    def add_rag(
            self,
            index: faiss.IndexFlatL2,
            embedding: SentenceTransformer,
            text_snippets: list[str],
            rag_top_k: int,
            rag_qa_base_question: Optional[str] = None
    ):
        self.index = index
        self.embedding = embedding
        self.text_snippets = text_snippets
        self.rag_top_k = rag_top_k
        self.rag_qa_base_question = rag_qa_base_question

    def sample(
            self,
            prompt: str,
            history: List[List[str]],
            system_prompt: str,
            mode: Literal["Chat", "Instruction"] = "Instruction",
            max_length: int = 4096,
            max_new_tokens: int = 4096,
            temperature: float = 0.8,
            top_p: float = 0.9,
            top_k: int = 50
    ):
        """
        The sample function is the main entry point for a user to interact with the model.
        It takes in a prompt, which can be any string, and returns an iterator over
        strings that are generated by the model.
        The sample function also takes in some optional arguments:

        :param self: Refer to the current object
        :param prompt: str: Pass in the text that you want to generate a response for
        :param history: List[List[str]]: Keep track of the conversation history
        :param system_prompt: str: the model system prompt.
        :param mode: str: represent the mode that model inference be used in (e.g chat or instruction)
        :param max_length: int: max_length for model
        :param max_new_tokens: int: Limit the number of tokens in a response
        :param temperature: float: Control the randomness of the generated text
        :param top_p: float: Control the probability of sampling from the top k tokens
        :param top_k: int: Control the number of candidates that are considered for each token
        :return: A generator that yields the next token in the sequence
        """
        assert mode in ["Chat", "Instruction"], "Requested Mode is not in Available Modes"
        if mode == "Instruction":
            history = []
        history.append([prompt, ""])
        index: faiss.IndexFlatL2 = self.index
        embedding: SentenceTransformer = self.embedding
        text_snippets: list[str] = self.text_snippets
        if index is not None and embedding is not None and embedding is not None:
            contexts = search(
                query=prompt,
                embedding=embedding,
                text_snippets=text_snippets,
                k=self.rag_top_k,
                index=index
            )

            prompt = self.interactor.retrival_qa_template(
                question=prompt,
                contexts=contexts,
                base_question=self.rag_qa_base_question
            )

            print(
                prompt
            )
        else:
            warnings.warn("You are not Using rag correctly you have to add rag via `add_rag` function")
        string = self.interactor.format_message(
            prompt=prompt,
            history=history,
            system_message=None if system_prompt == "" else system_prompt,
            prefix=self.interactor.get_prefix_prompt() if self.use_prefix_for_interactor else None,
        )

        total_response = ""

        streamer = TextIteratorStreamer(
            skip_prompt=True,
            tokenizer=self.tokenizer,
        )
        generation_config = self.generation_config
        generation_config.top_k = top_k
        generation_config.top_p = top_p
        generation_config.max_new_tokens = max_new_tokens
        generation_config.temperature = temperature
        inputs = dict(
            **self.tokenizer(string, return_tensors="pt").to(self.model.device),
            generation_config=generation_config,
            streamer=streamer,
            max_length=max_length
        )
        thread = Thread(target=self.model.generate, kwargs=inputs)
        thread.start()
        for char in streamer:
            total_response += char
            history[-1][-1] = total_response
            yield "", history
        thread.join()
