import gradio as gr
from ._theme import seafoam
from typing import List, Literal, Optional
from ...interactors import BaseInteract
from threading import Thread
from transformers import GenerationConfig, PreTrainedModel, TextIteratorStreamer, AutoTokenizer, PreTrainedTokenizerBase

js = """function () {
  gradioURL = window.location.href
  if (!gradioURL.endsWith("?__theme=dark")) {
    window.location.replace(gradioURL + "?__theme=dark");
  }
}"""


class BNBLLMServe:
    def __init__(
            self,
            interactor: BaseInteract,
            model: PreTrainedModel,
            tokenizer: AutoTokenizer | PreTrainedTokenizerBase,
            generation_config: GenerationConfig,
            use_prefix_for_interactor: bool = False,
            examples: Optional[list[str]] = None
    ):

        if examples is None:
            examples = [
                "Hello world",
                "What's up?",
                "Can you code?"
            ]
        tokenizer.pad_token_id = tokenizer.pad_token_id or tokenizer.eos_token_id
        self.interactor = interactor
        self.use_prefix_for_interactor = use_prefix_for_interactor
        self.model = model
        self.tokenizer = tokenizer
        self.generation_config = generation_config
        self.examples = examples

    def sample(
            self,
            prompt: str,
            history: List[List[str]],
            system_prompt: str,
            mode: Literal["Chat", "Instruction"] = "Instruction",
            max_length: int = 4096,
            max_new_tokens: int = 4096,
            temperature: float = 0.8,
            top_p: float = 0.9,
            top_k: int = 50
    ):
        """
        The sample function is the main entry point for a user to interact with the model.
        It takes in a prompt, which can be any string, and returns an iterator over
        strings that are generated by the model.
        The sample function also takes in some optional arguments:

        :param self: Refer to the current object
        :param prompt: str: Pass in the text that you want to generate a response for
        :param history: List[List[str]]: Keep track of the conversation history
        :param system_prompt: str: the model system prompt.
        :param mode: str: represent the mode that model inference be used in (e.g chat or instruction)
        :param max_length: int: max_length for model
        :param max_new_tokens: int: Limit the number of tokens in a response
        :param temperature: float: Control the randomness of the generated text
        :param top_p: float: Control the probability of sampling from the top k tokens
        :param top_k: int: Control the number of candidates that are considered for each token
        :return: A generator that yields the next token in the sequence
        """

        assert mode in ["Chat", "Instruction"], "Requested Mode is not in Available Modes"
        if mode == "Instruction":
            history = []
        string = self.interactor.format_message(
            prompt=prompt,
            history=history,
            system_message=None if system_prompt == "" else system_prompt,
            prefix=self.interactor.get_prefix_prompt() if self.use_prefix_for_interactor else None,
        )
        history.append([prompt, ""])
        total_response = ""

        streamer = TextIteratorStreamer(
            skip_prompt=True,
            tokenizer=self.tokenizer,
        )
        generation_config = self.generation_config
        generation_config.top_k = top_k
        generation_config.top_p = top_p
        generation_config.max_new_tokens = max_new_tokens
        generation_config.temperature = temperature
        inputs = dict(
            **self.tokenizer(string, return_tensors="pt").to(self.model.device),
            generation_config=generation_config,
            streamer=streamer,
            max_length=max_length
        )
        thread = Thread(target=self.model.generate, kwargs=inputs)
        thread.start()
        for char in streamer:
            total_response += char
            history[-1][-1] = total_response
            yield "", history
        thread.join()

    def chat_interface_components(self):
        """
        The function `chat_interface_components` creates the components for a chat interface, including
        a chat history, message box, buttons for submitting, stopping, and clearing the conversation,
        and sliders for advanced options.
        """
        with gr.Column("100%"):
            gr.Markdown(
                "# <h1><center style='color:white;'>Powered by "
                "[EasyDeL](https://github.com/erfanzar/EasyDel)</center></h1>",
            )
            history = gr.Chatbot(
                elem_id="EasyDel",
                label="EasyDel",
                container=True,
                height="65vh",
            )
            prompt = gr.Textbox(
                show_label=False, placeholder="Enter Your Prompt Here.", container=False
            )
            with gr.Row():
                submit = gr.Button(
                    value="Run",
                    variant="primary"
                )
                stop = gr.Button(
                    value="Stop"
                )
                clear = gr.Button(
                    value="Clear Conversation"
                )
            with gr.Accordion(open=False, label="Advanced Options"):
                system_prompt = gr.Textbox(
                    value="",
                    show_label=True,
                    label="System Prompt",
                    placeholder="System Prompt",
                    container=False
                )

                max_length = gr.Slider(
                    value=self.generation_config.max_length,
                    maximum=32768 * 2,
                    minimum=1,
                    label="Max Length",
                    step=1
                )
                max_new_tokens = gr.Slider(
                    value=self.generation_config.max_new_tokens,
                    maximum=10000,
                    minimum=1,
                    label="Max New Tokens",
                    step=1
                )
                temperature = gr.Slider(
                    value=self.generation_config.temperature,
                    maximum=1,
                    minimum=0.1,
                    label="Temperature",
                    step=0.01
                )
                top_p = gr.Slider(
                    value=self.generation_config.top_p,
                    maximum=1,
                    minimum=0.1,
                    label="Top P",
                    step=0.01
                )
                top_k = gr.Slider(
                    value=self.generation_config.top_k,
                    maximum=100,
                    minimum=1,
                    label="Top K",
                    step=1
                )
                mode = gr.Dropdown(
                    choices=["Chat", "Instruction"],
                    value="Chat",
                    label="Mode",
                    multiselect=False
                )

        inputs = [
            prompt,
            history,
            system_prompt,
            mode,
            max_length,
            max_new_tokens,
            temperature,
            top_p,
            top_k
        ]

        clear.click(fn=lambda: [], outputs=[history])
        sub_event = submit.click(
            fn=self.sample, inputs=inputs, outputs=[prompt, history]
        )
        txt_event = prompt.submit(
            fn=self.sample, inputs=inputs, outputs=[prompt, history]
        )

        stop.click(fn=None, inputs=None, outputs=None,
                   cancels=[txt_event, sub_event])

    def build_chat_interface(self) -> gr.Blocks:
        """
        The build function is the main entry point for your app.
        It should return a single gr.Block instance that will be displayed in the browser.

        :param self: Make the class methods work with an instance of the class
        :return: A block, which is then queued
        """
        with gr.Blocks(
                theme=seafoam,
                title="Chat",
                css="footer {visibility: hidden}"
        ) as block:
            self.chat_interface_components()
        block.queue()
        return block

    def build_inference(self) -> gr.Blocks:
        """
        The function "build_inference" returns a gr.Blocks object that contains two tabs, one for model
        interface components and one for chat interface components.
        :return: a gr.Blocks object.
        """
        with gr.Blocks(
                theme=seafoam,
                css="footer {visibility: hidden}"
        ) as block:
            with gr.Tab("Chat"):
                self.chat_interface_components()
        return block

    def __repr__(self):
        """
        The __repr__ function is used to generate a string representation of an object.
        This function should return a string that can be parsed by the Python interpreter
        to recreate the object. The __repr__ function is called when you use print() on an
        object, or when you type its name in the REPL.

        :param self: Refer to the instance of the class
        :return: A string representation of the object
        """
        string = f"{self.__class__.__name__}(\n"
        for k, v in self.__dict__.items():
            if not k.startswith("_"):
                repr_src = f"\t{k} : " + v.__str__().replace("\n", "\n\t") + "\n"
                string += repr_src if len(repr_src) < 500 else f"\t{k} : " + f"{v.__class__.__name__}(...)" + "\n"
        return string + ")"

    def __str__(self):
        """
        The __str__ function is called when you use the print function or when str() is used.
        It should return a string representation of the object.

        :param self: Refer to the instance of the class
        :return: The object"s string representation
        """
        return self.__repr__()
